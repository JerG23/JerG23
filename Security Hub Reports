import boto3
import pandas as pd
import os
import logging
import traceback
import re
from datetime import datetime

# --- Configuration ---
TARGET_PROFILES = [
    "ir-doi-gpsi", "doi-trimble-prod-com", "imagery-data-management", "nhd-plus",
    "doi-keycloak-prod-com-1", "doi-keycloak-prod-com-2", "ir-doi-osmre", "boem-odgip-prod-com",
    "fws-iris-mgmt-com", "fws-iris-prod-com", "fws-ecosphere-sandbox-com", "fws-ecosphere-common",
    "fws-ecosphere-dev", "fws-ecosphere-prod-com", "fws-ecosphere-test", "usgs-wim-wma", "gomcollab",
    "usgs-wim-prod", "usgs-wim-fws-cbrs", "usgs-wim-main", "usgs-wim-streamstats",
    "usgs-wim-fws-wetlands", "osmre-eamlis-prod-com", "usgs-shira-prod-com", "fws-tracs-prod",
    "bor-mp-gis", "doi-datainventory-dev-com", "fema-disasters-prod", "fema-disasters-sandbox",
    "fema-rmd-prod", "fema-rmd-1", "fema-rmd-2", "bia-bogs-prod-com", "nps-doi-1", "nps-doi-2",
    "nps-mapbox-prod-com", "nps-mapbox-dev-com", "nps-cartos-prod-com", "nps-cartos-dev-com",
    "nps-usmp-prod-com", "oas-safecom-staging", "doi-usdaiipp-prod-com", "usgs-trails-dev",
    "usgs-trails-prod", "doi-borgis-prod-com", "doi-ocio-ac-prod-com"
]
DEFAULT_REGION = "us-east-1"
OUTPUT_DIR = ""
LOG_LEVEL = logging.INFO

# --- Constants ---
MAX_SHEET_NAME_LENGTH = 30
SEVERITY_ORDER = {"CRITICAL": 1, "HIGH": 2, "MEDIUM": 3, "LOW": 4, "INFORMATIONAL": 5, "UNDEFINED": 6}
COLUMN_MAPPING = {
    'Id': 'Id',
    'Region': 'Region',
    'SeverityLabel': 'FindingProviderFields.Severity.Label',
    'WorkflowStatus': 'Workflow.Status',
    'ComplianceStatus': 'Compliance.Status',
    'GeneratorId': 'GeneratorId',
    'AwsAccountId': 'AwsAccountId',
    'Types': 'Types',
    'FirstObservedAt': 'FirstObservedAt',
    'LastObservedAt': 'LastObservedAt',
    'CreatedAt': 'CreatedAt',
    'UpdatedAt': 'UpdatedAt',
    'Title': 'Title',
    'Description': 'Description',
    'ImpactedResources': 'ImpactedResources',
    'RecordState': 'RecordState'
}
COLUMN_ORDER = list(COLUMN_MAPPING.keys())

# --- Logging Setup ---
_LOGGER = logging.getLogger(__name__)
_LOGGER.setLevel(LOG_LEVEL)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(levelname)s:%(asctime)s:%(name)s:%(message)s'))
if not _LOGGER.hasHandlers():
    _LOGGER.addHandler(handler)

def sanitize_sheet_name(name):
    return re.sub(r'[\\/*?:\[\]]', "_", name)[:MAX_SHEET_NAME_LENGTH]

def get_nested_value(obj, path, default=None):
    try:
        for key in path.split('.'):
            if isinstance(obj, dict):
                obj = obj.get(key)
            elif isinstance(obj, list) and key.isdigit():
                obj = obj[int(key)]
            else:
                return default
            if obj is None:
                return default
        return obj
    except Exception:
        return default

def get_enabled_regions_for_securityhub(session):
    try:
        ec2 = session.client('ec2', region_name=DEFAULT_REGION)
        regions = ec2.describe_regions(AllRegions=True)['Regions']
        enabled = []
        for region in regions:
            name = region['RegionName']
            try:
                sh = session.client('securityhub', region_name=name)
                sh.get_findings(MaxResults=1)
                enabled.append(name)
            except Exception:
                continue
        return enabled
    except Exception as e:
        _LOGGER.error(f"Error retrieving enabled regions: {e}")
        return [DEFAULT_REGION]

if __name__ == "__main__":
    _LOGGER.info("Starting Security Hub multi-region findings export...")

    if not TARGET_PROFILES:
        _LOGGER.warning("No profiles specified.")
        exit()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = os.path.join(OUTPUT_DIR, f"{timestamp}_security_hub_findings.xlsx")
    if OUTPUT_DIR:
        os.makedirs(OUTPUT_DIR, exist_ok=True)

    writer = pd.ExcelWriter(output_filename, engine='xlsxwriter')
    processed_accounts = 0

    for profile in TARGET_PROFILES:
        _LOGGER.info(f"Processing profile: {profile}")
        findings_data = []
        try:
            session = boto3.Session(profile_name=profile, region_name=DEFAULT_REGION)
            sts = session.client('sts')
            account_id = sts.get_caller_identity()['Account']
            try:
                iam = session.client('iam')
                aliases = iam.list_account_aliases().get('AccountAliases', [])
                account_alias = aliases[0] if aliases else account_id
            except:
                account_alias = account_id

            enabled_regions = get_enabled_regions_for_securityhub(session)
            _LOGGER.info(f"Regions enabled for Security Hub in {account_alias}: {enabled_regions}")

            for region in enabled_regions:
                _LOGGER.info(f"  Fetching findings from region {region}")
                sh = session.client('securityhub', region_name=region)
                paginator = sh.get_paginator('get_findings')
                pages = paginator.paginate(
                    Filters={'RecordState': [{'Value': 'ACTIVE', 'Comparison': 'EQUALS'}]},
                    PaginationConfig={'PageSize': 100}
                )

                for page in pages:
                    for finding in page.get('Findings', []):
                        workflow_status = get_nested_value(finding, 'Workflow.Status')
                        if workflow_status != 'NEW':
                            continue

                        row = {}
                        for col, path in COLUMN_MAPPING.items():
                            if col == 'Types':
                                val = get_nested_value(finding, path, [])
                                row[col] = ', '.join(val) if isinstance(val, list) else val
                            elif col == 'Region':
                                row[col] = region
                            elif col == 'ImpactedResources':
                                resources = finding.get('Resources', [])
                                impacted_ids = [res.get('Id') for res in resources if res.get('Id')]
                                row[col] = ', '.join(impacted_ids)
                            else:
                                row[col] = get_nested_value(finding, path)
                        findings_data.append(row)

            if findings_data:
                df = pd.DataFrame(findings_data)
                df = df.reindex(columns=COLUMN_ORDER)
                df['SeverityRank'] = df['SeverityLabel'].map(lambda x: SEVERITY_ORDER.get(str(x).upper(), 99))
                df.sort_values(by='SeverityRank', inplace=True)
                df.drop(columns=['SeverityRank'], inplace=True)

                sheet_name = sanitize_sheet_name(account_alias)
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                writer.sheets[sheet_name].autofit()
                _LOGGER.info(f"Wrote {len(df)} findings to sheet: {sheet_name}")
                processed_accounts += 1
            else:
                _LOGGER.info(f"No NEW findings found for account: {account_alias}")
                empty_df = pd.DataFrame(columns=COLUMN_ORDER)
                sheet_name = sanitize_sheet_name(account_alias)
                empty_df.to_excel(writer, sheet_name=sheet_name, index=False)

        except Exception as e:
            _LOGGER.error(f"Failed to process profile {profile}: {e}")
            _LOGGER.error(traceback.format_exc())

    try:
        writer.close()
        _LOGGER.info(f"Saved Excel report: {output_filename}")
    except Exception as e:
        _LOGGER.error(f"Failed to save report: {e}")

    _LOGGER.info(f"Processed {processed_accounts} out of {len(TARGET_PROFILES)} profiles.")
